[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Korch",
    "section": "",
    "text": "source\n\nHistory\nThis object inherits from base dict to provide a History object similar to Keras’, allowing the automatic logging of the loss and the different metrics during training. It’s automatically used in .fit() (as it is in Keras).\n\nsource\n\n\nModule\n\n Module (**kwargs)\n\nModification of PyTorch base nn.Module to provide a basic predefined training loop with logging and a Keras-like interface to be able to customize the training. This Module implements as well as a compile method and an evaluate one. All is done to obtain a behaviour as similar to Keras as possible.\n\n\nExample of usage\nWe can perform a very simple example using the Fashion MNIST dataset (as is done in the official PyTorch docs.\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# Create datasets for training & validation, download if necessary\ntraining_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\nvalidation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n\n# Create data loaders for our datasets; shuffle for training, not for validation\ntraining_loader = torch.utils.data.DataLoader(training_set, batch_size=128, shuffle=True, num_workers=2)\nvalidation_loader = torch.utils.data.DataLoader(validation_set, batch_size=256, shuffle=False, num_workers=2)\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\n\n\nSee that the only different with respect to basic PyTorch is that we’re inhereting from our custom Module, not from PyTorch’s nn.Module:\n\nclass SimpleModel(Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nFollowing the usual Keras way, we instantiate the model and compile it, providing the loss and the optimizer:\n\nmodel = SimpleModel()\nmodel.compile(loss=torch.nn.CrossEntropyLoss(),\n              optimizer=torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9),\n              metrics = MetricCollection([Accuracy()]))\n\n\nmodel\n\nSimpleModel(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=256, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n  (loss_fn): CrossEntropyLoss()\n  (metrics): MetricCollection(\n    (Accuracy): Accuracy()\n  )\n)\n\n\n\nmodel.evaluate(training_loader), model.evaluate(validation_loader)\n\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n\n\n\n\n\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n\n\n\n\n\n({'Accuracy': 0.1635905294847895, 'Loss': 2.3032673564292727},\n {'Accuracy': 0.15712890625, 'Loss': 2.3037723541259765})\n\n\n\nhistory = model.fit(trainloader=training_loader, epochs=1, validationloader=validation_loader)\n\n\n\n\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n\n\n\n\n\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n\n\n\n\n\n\nhistory\n\n{'Accuracy': [0.19986007462686567],\n 'Loss': [2.274188762534656],\n 'Val_Accuracy': [0.390625],\n 'Val_Loss': [2.169258713722229]}\n\n\n\nmodel.evaluate(training_loader), model.evaluate(validation_loader)\n\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n\n\n\n\n\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n\n\n\n\n\n({'Accuracy': 0.389570007128502, 'Loss': 2.1693943116202283},\n {'Accuracy': 0.390625, 'Loss': 2.169258713722229})"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Korch",
    "section": "",
    "text": "pip install korch"
  },
  {
    "objectID": "index.html#example-of-usage",
    "href": "index.html#example-of-usage",
    "title": "Korch",
    "section": "Example of usage",
    "text": "Example of usage\nWe can perform a very simple example using the Fashion MNIST dataset (as is done in the official PyTorch docs.\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# Create datasets for training & validation, download if necessary\ntraining_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\nvalidation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n\n# Create data loaders for our datasets; shuffle for training, not for validation\ntraining_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\nvalidation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)\n\nSee that the only different with respect to basic PyTorch is that we’re inhereting from our custom Module, not from PyTorch’s nn.Module:\n\nclass SimpleModel(Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nFollowing the usual Keras way, we instantiate the model and compile it, providing the loss and the optimizer. Metrics can be provided as well, and are expected as torchvision.MetricCollection:\n\nmodel = SimpleModel()\nmodel.compile(loss=torch.nn.CrossEntropyLoss(),\n              optimizer=torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9),\n              metrics = MetricCollection([Accuracy()]))\n\n\nmodel\n\nSimpleModel(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=256, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n  (loss_fn): CrossEntropyLoss()\n)\n\n\n\nmodel.evaluate(training_loader), model.evaluate(validation_loader)\n\n(2.3070596095879874, 2.307082461261749)\n\n\n\nmodel.fit(trainloader=training_loader, epochs=1, validationloader=validation_loader)\n\n\nmodel.evaluate(training_loader), model.evaluate(validation_loader)\n\n(0.39349932589025605, 0.42693415356237674)"
  }
]