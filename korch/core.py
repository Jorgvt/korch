# AUTOGENERATED! DO NOT EDIT! File to edit: 00_core.ipynb (unless otherwise specified).

__all__ = ['Module']

# Cell
#export
import torch
import torch.nn as nn
from tqdm.auto import tqdm

# Internal Cell
#exporti
def get_pbar_description_from_batch_metrics(batch_metrics, prefix=""):
    description = ""
    for name, value in batch_metrics.items():
        description += f'{prefix}{name}: {value:.2f} '
    return description[:-1]

# Cell
class Module(nn.Module):
    def __init__(self, **kwargs):
        super(Module, self).__init__(**kwargs)

    def train_step(self, batch):
        inputs, labels = batch
        self.optimizer.zero_grad()
        outputs = self(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()

        ## Obtain metrics if needed
        if self.metrics is not None:
            metrics = self.metrics(outputs, labels)
            metrics = {name:value.item() for name, value in metrics.items()}
            metrics['Loss'] = loss.item()
        else:
            metrics = {'Loss':loss.item()}
        return metrics

    def validation_step(self, batch):
        inputs, labels = batch
        outputs = self(inputs)
        loss = self.loss_fn(outputs, labels)

        ## Obtain metrics if needed
        if self.metrics is not None:
            metrics = self.metrics(outputs, labels)
            metrics = {name:value.item() for name, value in metrics.items()}
            metrics['Loss'] = loss.item()
        else:
            metrics = {'Loss':loss.item()}
        return metrics

    def fit(self, trainloader, epochs, validationloader=None):
        for epoch in tqdm(range(epochs), desc='Epochs', position=0):
            self.train()
            pbar = tqdm(enumerate(trainloader), total=len(trainloader), position=1, leave=False)
            for batch_idx, batch in pbar:
                batch_metrics = self.train_step(batch)
                pbar.set_description(get_pbar_description_from_batch_metrics(batch_metrics))
            if validationloader is not None:
                self.eval()
                pbar = tqdm(enumerate(validationloader), total=len(validationloader), position=2, leave=False)
                for batch_idx, batch in pbar:
                    with torch.no_grad():
                        batch_metrics = self.validation_step(batch)
                    pbar.set_description(get_pbar_description_from_batch_metrics(batch_metrics, 'Val_'))
            self.metrics.reset()

    def compile(self, loss=None, optimizer=None, metrics=None):
        """
        metrics: torchmetrics.MetricCollection
        """
        self.loss_fn = loss
        self.optimizer = optimizer
        self.metrics = metrics

    def evaluate(self, dataloader):
        self.eval()
        for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):
            if i == 0:
                results = self.validation_step(batch)
                results = {name:[value] for name, value in results.items()}
            else:
                result = self.validation_step(batch)
                for name, value in result.items():
                    results[name].append(value)
        results = {name:sum(values)/len(values) for name,values in results.items()}
        return results