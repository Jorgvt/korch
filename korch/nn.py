# AUTOGENERATED! DO NOT EDIT! File to edit: ../Notebooks/00_nn.ipynb.

# %% auto 0
__all__ = ['History', 'Module']

# %% ../Notebooks/00_nn.ipynb 3
from typing import Dict, Callable, Optional
from tqdm.auto import tqdm

import torch
import torch.nn as nn

from fastcore.basics import patch

# %% ../Notebooks/00_nn.ipynb 4
def get_pbar_description_from_batch_metrics(batch_metrics, prefix=""):
    description = ""
    for name, value in batch_metrics.items():
        description += f'{prefix}{name}: {value:.2f} '
    return description[:-1]

# %% ../Notebooks/00_nn.ipynb 5
class History(dict):
    """
    This object inherits from base `dict` to provide a `History` object similar to Keras', 
    allowing the automatic logging of the loss and the different metrics during training. 
    It's automatically used in `.fit()` (as it is in Keras).
    """
    def log_dict(self, 
                 data: Dict, # Dictionary to log.
                 prefix="", # Prefix for the logged metrics.
                 ):
        """Logs a dictionary into the `History` object."""
        for name, value in data.items():
            name = prefix+name
            if name in self.keys():
                self[name].append(value)
            else:
                self[name] = [value]

    def aggregate(self, 
                  agg_fn: Callable = lambda x: sum(x)/len(x), # Function used to aggregate the data.
                  ) -> Dict: # Returns a dictionary with the aggregated data.
        """Aggregates the stored values using the designed aggregation function."""
        return {name:agg_fn(values) for name, values in self.items()}

# %% ../Notebooks/00_nn.ipynb 6
class Module(nn.Module):
    """
    Modification of PyTorch base `nn.Module` to provide a basic
    predefined training loop with logging and a Keras-like interface
    to be able to customize the training.
    This Module implements as well a `.compile()` method and an `.evaluate()` one. 
    All is done to obtain a behaviour as similar to Keras as possible.
    """
    def __init__(self, **kwargs):
        super(Module, self).__init__(**kwargs)

# %% ../Notebooks/00_nn.ipynb 7
@patch
def train_step(self: Module, 
               batch, # Batch of data to train.
               ) -> Dict: # Metrics from the training step.
    """Perform a training step."""

    inputs, labels = batch
    self.optimizer.zero_grad()
    outputs = self(inputs)
    loss = self.loss_fn(outputs, labels)
    loss.backward()
    self.optimizer.step()

    ## Obtain metrics if needed
    if self.metrics is not None:
        metrics = self.metrics(outputs, labels)
        metrics = {name:value.item() for name, value in metrics.items()}
        metrics['Loss'] = loss.item()
    else:
        metrics = {'Loss':loss.item()}
    return metrics


# %% ../Notebooks/00_nn.ipynb 8
@patch
def validation_step(self: Module, 
                    batch, # Batch of data to validate.
                    ) -> Dict: # Metrics from the validation step.
    """Perform a validation step"""

    inputs, labels = batch
    outputs = self(inputs)
    loss = self.loss_fn(outputs, labels)

    ## Obtain metrics if needed
    if self.metrics is not None:
        metrics = self.metrics(outputs, labels)
        metrics = {name:value.item() for name, value in metrics.items()}
        metrics['Loss'] = loss.item()
    else:
        metrics = {'Loss':loss.item()}
    return metrics

# %% ../Notebooks/00_nn.ipynb 9
@patch
def fit(self: Module, 
        trainloader: torch.utils.data.DataLoader, # Training DataLoader. 
        epochs, # Number of epochs to train.
        validationloader=None, # Validation DataLoader (optional).
        ) -> History: # History object with the training dynamics as in Keras.
    """Fit a model to the desired `trainloader` for `epochs` epochs. Returns the corresponding `History`."""

    history_epoch = History()
    for epoch in tqdm(range(epochs), desc='Epochs', position=0):
        self.train()
        pbar = tqdm(enumerate(trainloader), total=len(trainloader), position=1, leave=False)
        history_batch = History()
        for batch_idx, batch in pbar:
            batch_metrics = self.train_step(batch)
            history_batch.log_dict(batch_metrics)
            pbar.set_description(get_pbar_description_from_batch_metrics(batch_metrics))
        if validationloader is not None:
            self.eval()
            pbar = tqdm(enumerate(validationloader), total=len(validationloader), position=2, leave=False)
            for batch_idx, batch in pbar:
                with torch.no_grad():
                    batch_metrics = self.validation_step(batch)
                history_batch.log_dict(batch_metrics, prefix='Val_')
                pbar.set_description(get_pbar_description_from_batch_metrics(batch_metrics, 'Val_'))
        self.metrics.reset()
        history_epoch.log_dict(history_batch.aggregate())
    return history_epoch

# %% ../Notebooks/00_nn.ipynb 10
@patch
def compile(self: Module, 
            loss: Optional=None, # Loss function to be used.
            optimizer: Optional[torch.optim.Optimizer] = None, # Optimizer to be used.
            metrics=None, # `MetricCollection` containing the desired metrics.
            ):
    """Sets the loss, optimizer and desired metrics to train the model."""

    self.loss_fn = loss
    self.optimizer = optimizer
    self.metrics = metrics

# %% ../Notebooks/00_nn.ipynb 11
@patch
def evaluate(self: Module, 
             dataloader, # `DataLoader` to evaluate the model with.
             ) -> Dict: # Results from the evaluation
    """Evaluates the model on a set of data."""

    self.eval()
    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):
        if i == 0:
            results = self.validation_step(batch)
            results = {name:[value] for name, value in results.items()}
        else:
            result = self.validation_step(batch)
            for name, value in result.items():
                results[name].append(value)
    results = {name:sum(values)/len(values) for name,values in results.items()}
    return results
