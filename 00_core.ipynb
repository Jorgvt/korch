{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korch\n",
    "\n",
    "> Simple tools to provide a Keras-like interface to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, MetricCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Module\n",
    "\n",
    "This Module implements a basic training loop similar to Keras', as well as a `compile` method and an `evaluate` one. All is done to obtain a behaviour as similar to Keras as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pbar_description_from_batch_metrics(batch_metrics, prefix=\"\"):\n",
    "    description = \"\"\n",
    "    for name, value in batch_metrics.items():\n",
    "        description += f'{prefix}{name}: {value:.2f} '\n",
    "    return description[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Module, self).__init__(**kwargs)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        self.train()\n",
    "\n",
    "        inputs, labels = batch\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        ## Obtain metrics if needed\n",
    "        if self.metrics is not None:\n",
    "            metrics = self.metrics(outputs, labels)\n",
    "            metrics = {name:value.item() for name, value in metrics.items()}\n",
    "            metrics['Loss'] = loss.item()\n",
    "        else:\n",
    "            metrics = {'Loss':loss.item()}\n",
    "        return metrics\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        self.eval()\n",
    "\n",
    "        inputs, labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = self(inputs)\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        ## Obtain metrics if needed\n",
    "        if self.metrics is not None:\n",
    "            metrics = self.metrics(outputs, labels)\n",
    "            metrics = {name:value.item() for name, value in metrics.items()}\n",
    "            metrics['Loss'] = loss.item()\n",
    "        else:\n",
    "            metrics = {'Loss':loss.item()}\n",
    "        return metrics\n",
    "\n",
    "    def fit(self, trainloader, epochs, validationloader=None):\n",
    "        for epoch in tqdm(range(epochs), desc='Epochs', position=0):\n",
    "            pbar = tqdm(enumerate(trainloader), total=len(trainloader), position=1, leave=False)\n",
    "            for batch_idx, batch in pbar:\n",
    "                batch_metrics = self.train_step(batch)\n",
    "                pbar.set_description(get_pbar_description_from_batch_metrics(batch_metrics))\n",
    "            if validationloader is not None:\n",
    "                pbar = tqdm(enumerate(validationloader), total=len(validationloader), position=2, leave=False)\n",
    "                for batch_idx, batch in pbar:\n",
    "                    batch_metrics = self.validation_step(batch)\n",
    "                    pbar.set_description(get_pbar_description_from_batch_metrics(batch_metrics, 'Val_'))\n",
    "            self.metrics.reset()\n",
    "\n",
    "    def compile(self, loss=None, optimizer=None, metrics=None):\n",
    "        \"\"\"\n",
    "        metrics: torchmetrics.MetricCollection\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        results = [self.validation_step(batch) for batch in tqdm(dataloader, total=len(dataloader))]\n",
    "        return sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a very simple example using the Fashion MNIST dataset (as is done in the official [PyTorch docs](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the only different with respect to basic PyTorch is that we're inhereting from our custom `Module`, not from PyTorch's `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the usual Keras way, we instantiate the model and compile it, providing the *loss* and the *optimizer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "model.compile(loss=torch.nn.CrossEntropyLoss(),\n",
    "              optimizer=torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "              metrics = MetricCollection([Accuracy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (metrics): MetricCollection(\n",
       "    (Accuracy): Accuracy()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.304426094388962, 2.304496095752716)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(training_loader), model.evaluate(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63783f1a73e243e0a22e8f867964146b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d7284cbbcf412faa02c6201643409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517a764da5fc4ca9b9a2f393a8c7c769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a798d1413345b98656b61e3020c0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37740d3ba4564544a94f824ad63e94cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(trainloader=training_loader, epochs=2, validationloader=validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3812980049278975, 0.41120041666950563)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(training_loader), model.evaluate(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuda')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
